{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import openai\n",
    "import os\n",
    "import sklearn.metrics\n",
    "import random\n",
    "import time\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxx\"\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_utils import TASKS, load_data, load_prompt, generate_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your LLM stack goes here\n",
    "\n",
    "\n",
    "# example of inference function for penai models\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "GPT_TURBO = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=600)\n",
    "\n",
    "def call_llm_openai(prompt):\n",
    "    output = GPT_TURBO([HumanMessage(content=prompt)])\n",
    "    return output.content\n",
    "\n",
    "\n",
    "# example of inference function for modal hosted models\n",
    "import requests\n",
    "\n",
    "def call_llm_modal(prompt):\n",
    "    r = requests.post('https://xxxxxxxxx.modal.run', json={'question': prompt})\n",
    "    output_dict = r.json()\n",
    "    output = output_dict['output']\n",
    "    completion = output[len(prompt):].strip()\n",
    "    return completion\n",
    "\n",
    "\n",
    "# example of inference function for baseten hosted models\n",
    "import baseten\n",
    "MODEL = baseten.deployed_model_version_id('xxxxx')\n",
    "\n",
    "def call_llm_baseten(prompt):\n",
    "    output = MODEL.predict({\"prompt\": prompt, \"do_sample\": True, \"max_new_tokens\": 300})\n",
    "    completion = output['data']['generated_text'][len(prompt):].strip()\n",
    "    return completion\n",
    "\n",
    "\n",
    "call_llm = call_llm_modal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tasks: list, tasks_dir: str):\n",
    "    report = dict()\n",
    "    for task in tasks:\n",
    "        train_df, test_df = load_data(task=task, tasks_dir=tasks_dir)\n",
    "        prompt_template = load_prompt(prompt_name=\"base_prompt.txt\", task=task, tasks_dir=tasks_dir)\n",
    "        prompts = generate_prompts(prompt_template=prompt_template, data_df=train_df)\n",
    "        report[task] = dict()\n",
    "        targets = list()\n",
    "        outputs = list()\n",
    "        print('task', task)\n",
    "        for prompt, data in zip(prompts, train_df.iterrows()):\n",
    "            datapoint_id, data = data\n",
    "            output = call_llm(prompt)\n",
    "            output = output.strip()\n",
    "            targets.append(data['answer'])\n",
    "            outputs.append(output)\n",
    "            success = output == data['answer']\n",
    "            report[task][datapoint_id] = {\n",
    "                'prompt': prompt,\n",
    "                'generated_output': output,\n",
    "                'correct_output': data['answer'],\n",
    "                'success': output == data['answer']\n",
    "            }\n",
    "        report[task]['balanced_accuracy'] = sklearn.metrics.balanced_accuracy_score(targets, outputs)\n",
    "        print('task balanced accuracy:', report[task]['balanced_accuracy'])\n",
    "        print()\n",
    "    \n",
    "    print('Total Balanced Accuracy:', sum([report[task]['balanced_accuracy'] if not math.isnan(report[task]['balanced_accuracy']) else 0 for task in tasks])/len(tasks))\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task maud_change_in_law:__subject_to_\"disproportionate_impact\"_modifier\n",
      "<Response [200]>\n",
      "request took 3.805708885192871\n",
      "task balanced accuracy: 1.0\n",
      "\n",
      "task maud_fiduciary_exception:__board_determination_standard\n",
      "<Response [200]>\n",
      "request took 1.0872490406036377\n",
      "task balanced accuracy: 1.0\n",
      "\n",
      "task contract_nli_notice_on_compelled_disclosure\n",
      "<Response [200]>\n",
      "request took 147.55259203910828\n",
      "<Response [200]>\n",
      "request took 152.9378478527069\n",
      "<Response [200]>\n",
      "request took 151.59520721435547\n",
      "<Response [200]>\n",
      "request took 154.38736081123352\n",
      "<Response [200]>\n",
      "request took 152.96601271629333\n",
      "<Response [200]>\n",
      "request took 180.8879361152649\n",
      "<Response [200]>\n",
      "request took 153.9524908065796\n",
      "<Response [200]>\n",
      "request took 160.44596099853516\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task diversity_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguel/.virtualenvs/legalbencheval/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "request took 133.1644549369812\n",
      "<Response [200]>\n",
      "request took 134.0045039653778\n",
      "<Response [200]>\n",
      "request took 131.24257731437683\n",
      "<Response [200]>\n",
      "request took 145.99635815620422\n",
      "<Response [200]>\n",
      "request took 133.32334804534912\n",
      "<Response [200]>\n",
      "request took 133.69695782661438\n",
      "task balanced accuracy: 0.0\n",
      "\n",
      "task opp115_third_party_sharing_collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguel/.virtualenvs/legalbencheval/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "request took 115.00699591636658\n",
      "<Response [200]>\n",
      "request took 111.51435327529907\n",
      "<Response [200]>\n",
      "request took 112.91970610618591\n",
      "<Response [200]>\n",
      "request took 113.22303605079651\n",
      "<Response [200]>\n",
      "request took 120.68412208557129\n",
      "<Response [200]>\n",
      "request took 112.27995800971985\n",
      "<Response [200]>\n",
      "request took 123.01961588859558\n",
      "<Response [200]>\n",
      "request took 195.04768300056458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguel/.virtualenvs/legalbencheval/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task balanced accuracy: 0.0\n",
      "\n",
      "Total Balanced Accuracy: 0.4\n"
     ]
    }
   ],
   "source": [
    "tasks_dir = '../legalbench'\n",
    "\n",
    "# warnings are to be expected\n",
    "\n",
    "random.seed(0)\n",
    "report = evaluate(tasks=random.sample(TASKS, 5), tasks_dir=tasks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legalbencheval",
   "language": "python",
   "name": "legalbencheval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
